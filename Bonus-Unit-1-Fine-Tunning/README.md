# ğŸ Bonus Unit 1: Fine-Tuning an LLM for Function-Calling

![Fine Tuning LLM for Function-Calling](../assets/bonus1thumbnail.jpg)

---

## ğŸ§  Introduction

Function-calling is an essential technique in modern LLM applications. Unlike prompt-only models, a function-calling model is trained to **decide when to act**, **call external tools**, and **interpret their results**.  
This unit goes beyond what you learned in Unit 1 and shows how to build such models.

---

## â³ When Should I Take This?

<details>
<summary>ğŸ“Œ Click to find out</summary>

This unit is optional but highly valuable. We recommend taking it if:

- You've completed Unit 1 and understand prompt-based systems.
- You want your LLM to actively interact with tools via APIs.
- You're comfortable with basic fine-tuning using Hugging Face.

Donâ€™t worry if youâ€™re new to some concepts â€” we link all required resources.

ğŸ“– Prerequisites:
- âœ… [Fine-Tune an LLM](https://huggingface.co/course/chapter3/3?fw=pt)
- âœ… [SFTTrainer Docs](https://huggingface.co/docs/trl/main/en/sft_trainer)
</details>

---

## ğŸ“š Unit Progress

```markdown
### Core Concepts
- [x] [Introduction](./README.md)
- [x] [What is Function Calling?](./Function-Calling.md)
- [x] [Fine-Tune for Function-Calling](./Fine-Tune-for-Function-Calling.md)
- [x] [Hands-on Implementation](./bonus_unit1.ipynb)
```

---

## ğŸš€ What Youâ€™ll Learn

### ğŸ§© Function Calling  
Modern LLMs can structure conversations to trigger **external Tools/APIs** like weather checkers or search engines.  
ğŸ“„ [Learn more â†’](./Function-Calling.md)

---

### âš™ï¸ LoRA (Low-Rank Adaptation)  
A technique to fine-tune large models **efficiently** by updating only a small number of parameters.  
ğŸ”— [LoRA Concept Guide](https://huggingface.co/docs/peft/conceptual_guides/lora)

---

### ğŸ§  Thought â†’ Act â†’ Observe Cycle  
A structured reasoning flow:
1. **Thought** â€“ Internal reasoning  
2. **Act** â€“ Calls external function/tool  
3. **Observe** â€“ Handles the result/output

<details>
<summary>ğŸ§ª Example Format</summary>

```yaml
user: "Get me today's weather in Paris."
model:
  function_call:
    name: getWeather
    arguments:
      city: "Paris"
```

</details>

---

### ğŸ·ï¸ Special Tokens  
Used to separate:
- ğŸ’­ Internal thoughts (`<thought>`)
- ğŸ”§ Function calls (`<function>`)
- ğŸ“© Observations/results (`<observation>`)

This helps the model distinguish reasoning from external actions.

---

## ğŸ¯ Learning Outcomes

By completing this unit, you will:

âœ… Understand how APIs interact with LLMs  
âœ… Fine-tune an instruction-following model using LoRA  
âœ… Apply the Thought â†’ Act â†’ Observe cycle in training  
âœ… Use special tokens to enhance structured reasoning  
âœ… Build a fully fine-tuned model for function-calling ğŸ”¥

---

## ğŸ“‚ Implementation Files

| File | Description |
|------|-------------|
| [bonus_unit1.ipynb](./bonus_unit1.ipynb) | Hands-on notebook |
| [Function-Calling.md](./Function-Calling.md) | Concept overview |
| [Fine-Tune-for-Function-Calling.md](./Fine-Tune-for-Function-Calling.md) | Training guide |

---

## ğŸ” Additional Resources

- ğŸ§  [Hugging Face Transformers Course](https://huggingface.co/course)
- ğŸ“˜ [LoRA Documentation](https://huggingface.co/docs/peft/conceptual_guides/lora)
- ğŸ§ª [SFTTrainer Documentation](https://huggingface.co/docs/trl/main/en/sft_trainer)

---

## ğŸ› ï¸ Prerequisites

To run the code:

- Python 3.8+
- PyTorch
- `transformers`
- `peft`
- `trl`

You can install with:

```bash
pip install torch transformers peft trl
```

---

## ğŸ“ Notes

- All code is in the Jupyter Notebook
- Documentation is split into linked Markdown files
- LoRA is used for efficient fine-tuning

---

### âœ… Final Checklist

```markdown
- [x] Understand Function Calling
- [x] Learn about LoRA
- [x] Apply Thought â†’ Act â†’ Observe
- [x] Fine-tune using Hugging Face tools
- [x] Complete the hands-on notebook
```
```